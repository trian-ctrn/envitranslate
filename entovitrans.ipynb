{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entovitrans.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNR+xTXn9RvM2HwYYszwlx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trian-ctrn/envitranslate/blob/main/entovitrans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "l98c2Isnjg6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "XU8DVEi5j0MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vncorenlp"
      ],
      "metadata": {
        "id": "UzWXavxb2FfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM , pipeline\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import sentencepiece\n",
        "import time\n",
        "import torch\n",
        "from vncorenlp import VnCoreNLP\n",
        "from nltk.corpus import wordnet\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "nltk.download('word2vec_sample')"
      ],
      "metadata": {
        "id": "0LiCyEJfjOOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
      ],
      "metadata": {
        "id": "zelSdVTyq5uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone  https://github.com/vncorenlp/VnCoreNLP\n"
      ],
      "metadata": {
        "id": "rGXEHXqXsLPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotator = VnCoreNLP(\"/content/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos\")"
      ],
      "metadata": {
        "id": "WLGPrvIN2EpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
        "\n",
        "model_pos = AutoModelForTokenClassification.from_pretrained(\"vblagoje/bert-english-uncased-finetuned-pos\")  \n"
      ],
      "metadata": {
        "id": "RzGTfOby8SBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vietnamese_postag(input):\n",
        "  annotated_text = annotator.annotate(input)\n",
        "  target_dict = []\n",
        "  for first_dict in annotated_text.values():\n",
        "    for another_dict in first_dict:\n",
        "      for another_another_dict in another_dict:\n",
        "        res = {key: another_another_dict[key] for key in another_another_dict.keys()\n",
        "                               & {'form','posTag,ner'}} \n",
        "        target_dict.append(res)\n",
        "\n",
        "  split_text = []\n",
        "  pos_tag = []\n",
        "  text = []\n",
        "  for i in range(len(target_dict)):\n",
        "    split_text.append(target_dict[i]['form'])\n",
        "    pos_tag.append(target_dict[i]['posTag'])\n",
        "  for i in range(len(split_text)):\n",
        "    text.extend([(split_text[i],pos_tag[i]),(\" \",None)])\n",
        "  return text"
      ],
      "metadata": {
        "id": "wv_0275g_5j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkK7tfy3iMYr"
      },
      "outputs": [],
      "source": [
        "model_path_en2vi = \"Yama/yamavi\"\n",
        "model_path_vi2en = \"Yama/yamaen\"\n",
        "def load_model(model_path_en2vi,model_path_vi2en):\n",
        "    tokenizer_en2vi = AutoTokenizer.from_pretrained(model_path_en2vi)\n",
        "    model_en2vi = AutoModelForSeq2SeqLM.from_pretrained(model_path_en2vi)\n",
        "    tokenizer_vi2en = AutoTokenizer.from_pretrained(model_path_vi2en)\n",
        "    model_vi2en = AutoModelForSeq2SeqLM.from_pretrained(model_path_vi2en)      \n",
        "    return model_en2vi, tokenizer_en2vi,model_vi2en,tokenizer_vi2en\n",
        "model_entovi , tokenizer_entovi, model_vitoen, tokenizer_vitoen  = load_model(model_path_en2vi,model_path_vi2en) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = {\n",
        "    \"0\": \"ADJ\",\n",
        "    \"1\": \"ADP\",\n",
        "    \"2\": \"ADV\",\n",
        "    \"3\": \"AUX\",\n",
        "    \"4\": \"CCONJ\",\n",
        "    \"5\": \"DET\",\n",
        "    \"6\": \"INTJ\",\n",
        "    \"7\": \"NOUN\",\n",
        "    \"8\": \"NUM\",\n",
        "    \"9\": \"PART\",\n",
        "    \"10\": \"PRON\",\n",
        "    \"11\": \"PROPN\",\n",
        "    \"12\": \"PUNCT\",\n",
        "    \"13\": \"SCONJ\",\n",
        "    \"14\": \"SYM\",\n",
        "    \"15\": \"VERB\",\n",
        "    \"16\": \"X\"\n",
        "  }\n"
      ],
      "metadata": {
        "id": "ltwaz34rwxtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def entovi(input):  \n",
        "    tokenized_txt = tokenizer_entovi(input,return_tensors = 'pt').input_ids\n",
        "    outputs = model_entovi.generate(tokenized_txt)\n",
        "    return tokenizer_entovi.decode(outputs[0], skip_special_tokens = True)\n",
        "def vitoen(input):  \n",
        "    tokenized_txt = tokenizer_entovi(input,return_tensors = 'pt').input_ids\n",
        "    outputs = model_vitoen.generate(tokenized_txt)\n",
        "    return tokenizer_vitoen.decode(outputs[0], skip_special_tokens = True)\n",
        "def english_postag(input):\n",
        "  label_id = []\n",
        "  split_text = input.split(' ')\n",
        "  inputs = tokenizer(input, add_special_tokens=True, return_tensors=\"pt\")\n",
        "  with torch.no_grad():\n",
        "    logits = model_pos(**inputs).logits\n",
        "\n",
        "  predicted_token_class_ids = logits.argmax(-1)\n",
        "  for i in predicted_token_class_ids[0]:\n",
        "      label_id.append(label[str(i.item())])\n",
        "  del(label_id[0])\n",
        "  del(label_id[-1])\n",
        "  text = []\n",
        "  for i in range(len(split_text)):\n",
        "    text.extend([(split_text[i], label_id[i]),(\" \",None)])\n",
        "  return text\n",
        "def vietnamese_postag(input):\n",
        "  annotated_text = annotator.annotate(input)\n",
        "  target_dict = []\n",
        "  for first_dict in annotated_text.values():\n",
        "    for another_dict in first_dict:\n",
        "      for another_another_dict in another_dict:\n",
        "        res = {key: another_another_dict[key] for key in another_another_dict.keys()\n",
        "                               & {'form','posTag'}} \n",
        "        target_dict.append(res)\n",
        "\n",
        "  split_text = []\n",
        "  pos_tag = []\n",
        "  text = []\n",
        "  for i in range(len(target_dict)):\n",
        "    split_text.append(target_dict[i]['form'])\n",
        "    pos_tag.append(target_dict[i]['posTag'])\n",
        "  for i in range(len(split_text)):\n",
        "    text.extend([(split_text[i],pos_tag[i]),(\" \",None)])\n",
        "  return text\n",
        "def greet(Choose,input):#,audio,state=\"\"):\n",
        "    if Choose == \"Eng to Vi\":\n",
        "      return vietnamese_postag(entovi(input)) \n",
        "    if Choose == \"Vi to Eng\":\n",
        "      return english_postag(vitoen(input)) \n",
        "def w2em(list_input):\n",
        "  synonyms = model.most_similar(positive=list_input.split(' '), topn = 10)\n",
        "  words = []\n",
        "  for i in range(10):\n",
        "    words.append(synonyms[i][0])\n",
        "  return \", \".join(words)"
      ],
      "metadata": {
        "id": "MIJXRn0zmbpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(fn = w2em, \n",
        "                         inputs = [gr.Textbox(placeholder=\"Example: King Woman Royal\")],\n",
        "                                            #  \"number\"],\n",
        "                        outputs = [gr.Textbox()]\n",
        "                     )\n"
      ],
      "metadata": {
        "id": "MAuglQFaw4ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(fn = greet,\n",
        "                     inputs = [gr.inputs.Dropdown([\"Eng to Vi\", \"Vi to Eng\"]),\n",
        "                               gr.Textbox(placeholder=\"Enter sentence here...\")],\n",
        "                               #gr.inputs.Audio(source=\"microphone\",type=\"filepath\")],\n",
        "                     outputs=[\"highlight\"],\n",
        "                     description = \"Write a full short sentence(maximum length = 80), shouldn't write a word\"\n",
        "                     )\n"
      ],
      "metadata": {
        "id": "UW9lsoqKmuEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hola = gr.TabbedInterface([demo,iface], [\"Advanced Search\", \"Translate\"])\n",
        "hola.launch()"
      ],
      "metadata": {
        "id": "D0PSdYHCpVx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pQWTcJ69AkN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}